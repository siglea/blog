---
layout: post
title:  "Netty"
date:   2020-05-18 23:25:00 +0900
comments: true
tags:
- 网络
- Java
 
categories:
- 技术
---


#### Netty 要点
- NioEventLoopGroup、EventLoop都是集成或者实现 ScheduledExecutorService、ExecutorService，因此本质都是线程池
<https://www.jianshu.com/p/da4398743b5a>
- NioEventLoopGroup还提供了setIoRatio()和rebuildSelectors()两个方法，一个用来设置I/O任务和非I/O任务的执行时间比，一个用来重建线程中的selector来规避JDK的epoll 100% CPU Bug。其实现也是依次设置各线程的状态，故不再列出。
- Java NIO支持HeapByteBuffer与DirectByteBuffer，Netty使用DirectByteBuffer实现零拷贝
```shell
ByteBuffer buffer = ByteBuffer.allocate(int capacity);
ByteBuffer buffer = ByteBuffer.allocateDirect(int capacity);
```
- OioServerSocketChannel 是同步阻塞 IO 的服务端实现，它接受新的客户端连接，并为它们创建 OioSocketChannel。
- BIO  Stream单向 与 NIO Channel双向
ChannelInboundHandler SimpleChannelInboundHandler

#### Netty Reactor多种模式, Reactor模式又有别名“Dispatcher”或者“Notifier”
- Reactor 是反应堆的意思，Reactor 模型是指通过一个或多个输入同时传递给服务处理器的服务请求的事件驱动处理模式。
服务端程序处理传入多路请求，并将它们同步分派给请求对应的处理线程，Reactor 模式也叫 Dispatcher 模式，即 I/O 多了复用统一监听事件，收到事件后分发(Dispatch 给某进程)，是编写高性能网络服务器的必备技术之一。

```java
package com.crossoverjie.cim.server.server;

import com.crossoverjie.cim.common.constant.Constants;
import com.crossoverjie.cim.common.protocol.CIMRequestProto;
import com.crossoverjie.cim.server.init.CIMServerInitializer;
import com.crossoverjie.cim.server.util.SessionSocketHolder;
import com.crossoverjie.cim.server.vo.req.SendMsgReqVO;
import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.nio.NioServerSocketChannel;
import io.netty.channel.socket.nio.NioSocketChannel;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;
import java.net.InetSocketAddress;

@Component
public class CIMServer {

    public void start() throws InterruptedException {
        // 单线程Reactor配置
        EventLoopGroup singleEventLoopGroup = new NioEventLoopGroup();
        EventLoopGroup singleEventLoop = singleEventLoopGroup.next();
        ServerBootstrap bootstrapSingle = new ServerBootstrap()
                .group(singleEventLoop, singleEventLoop)
                .channel(NioServerSocketChannel.class)
                .localAddress(new InetSocketAddress(nettyPort))
                //保持长连接
                .childOption(ChannelOption.SO_KEEPALIVE, true)
                // Nagle拆包算法禁用
                .childOption(ChannelOption.TCP_NODELAY, true)
                // sync-recv个数，防止sync-blood攻击
                .childOption(ChannelOption.SO_BACKLOG,1024)
                .childHandler(new CIMServerInitializer());

        // 单Boss线程，多Worker线程，Reactor配置
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup();
        EventLoopGroup bossEventLoop = eventLoopGroup.next();
        EventLoopGroup workLoopGroup = new NioEventLoopGroup();
        ServerBootstrap bootstrapSingle2 = new ServerBootstrap()
                .group(bossEventLoop, workLoopGroup)
                .channel(NioServerSocketChannel.class)
                .localAddress(new InetSocketAddress(nettyPort))
                .childOption(ChannelOption.SO_KEEPALIVE, true)
                .childOption(ChannelOption.SO_BACKLOG,1024)
                .childHandler(new CIMServerInitializer());

        // 多Boss线程，多Worker线程，Reactor配置
        EventLoopGroup boss = new NioEventLoopGroup();
        EventLoopGroup work = new NioEventLoopGroup();
        ServerBootstrap bootstrap = new ServerBootstrap()
                .group(boss, work)
                .channel(NioServerSocketChannel.class)
                .localAddress(new InetSocketAddress(nettyPort))
                .childOption(ChannelOption.SO_KEEPALIVE, true)
                .childOption(ChannelOption.SO_BACKLOG,1024)
                .childHandler(new CIMServerInitializer());

        ChannelFuture future = bootstrap.bind().sync();
        if (future.isSuccess()) {
            LOGGER.info("启动 cim server 成功");
        }
    }
}
```

- <https://mp.weixin.qq.com/s/66TxgxynqlQ1978_qMFSwQ>
- <https://github.com/code4craft/netty-learning/blob/master/posts/ch4-reactor.md>
- <https://mp.weixin.qq.com/s/csslzxEGTRX1WnK5Qp8jWQ>
- <http://www.52im.net/forum.php?mod=viewthread&tid=2043&highlight=netty>

#### Netty的零拷贝，数据最终只有一份在kernel buffer
<pre>
1. ReadBuffer copyTo AppBuffer
2. AppBuffer  copyTo SocketBuffer
3. SocketBuffer  copyTo  NIC Buffer
通过transferTo()，减少了从内核态copyTo用户态，演变为
1. ReadBuffer copyTo SocketBuffer
3. SocketBuffer  copyTo  NIC Buffer
通过sendfile，去除了SocketBuffer，演变为 (Linux 2.1)
1. ReadBuffer copyTo  NIC Buffer
根据socket buffer中的位置和偏移量直接将kernel buffer的数据copy到网卡设备（protocol engine）中，演变为 (Linux 2.4)
1. 根据位置和偏移量，   NIC Buffer，直接从ReadBuffer读取
</pre>

- DMA(Direct Memory Access，直接存储器访问)
- <https://www.jianshu.com/p/017f193663a0>

#### Protocol Buffer 的序列化 & 反序列化简单 & 速度快的原因是:
1. 编码 / 解码 方式简单(只需要简单的数学运算 = 位移等等)
2. 采用 Protocol Buffer 自身的框架代码 和 编译器 共同完成

#### Protocol Buffer 的数据压缩效果好(即序列化后的数据量体积小)的原因是:
1. a. 采用了独特的编码方式，如 Varint、Zigzag 编码方式等等
2. b. 采用 T - L - V 的数据存储方式:减少了分隔符的使用 & 数据存储得紧凑

<<<<<<< HEAD
=======
#### TCP 粘包/分包的原因:
- 应用程序写入的字节大小大于套接字发送缓冲区的大小，会发生拆包现象，而应用程序写入 数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发生粘包现 象;
- 进行 MSS 大小的 TCP 分段，当 TCP 报文长度-TCP 头部长度>MSS 的时候将发生拆包 以太网帧的 payload(净荷)大于 MTU(1500 字节)进行 ip 分片。
- 解决方案
    - FixedLengthFrameDecoder 客户端在发送数据包的时候，每个包都固定长度，比如1024个字节大小，如果客户端发送的数据长度不足1024个字节，则通过补充空格的方式补全到指定长度；
    - LineBasedFrameDecoder/DelimiterBasedFrameDecoder 客户端在每个包的末尾使用固定的分隔符，例如\r\n，如果一个包被拆分了，则等待下一个包发送过来之后找到其中的\r\n，然后对其拆分后的头部部分与前一个包的剩余部分进行合并，这样就得到了一个完整的包；
    - LengthFieldBasedFrameDecoder 将消息分为头部和消息体，在头部中保存有当前整个消息的长度，只有在读取到足够长度的消息之后才算是读到了一个完整的消息；
```shell 
ch.pipeline().addLast(new FixeLengthFrameDecoder(31))
```
    
#### 从零开发一个IM服务端  
- 通俗易懂 <http://www.52im.net/forum.php?mod=viewthread&tid=2768&highlight=netty>
- 基于Netty实现海量接入的推送服务技术要点 <http://www.52im.net/forum.php?mod=viewthread&tid=166&highlight=netty>

>>>>>>> gogogo
#### BIO,NIO,AIO 有什么区别?
- BIO：Block IO 同步阻塞式 IO，就是我们平常使用的传统 IO，它的特点是模式简单使用方便，并发处理能力低。
- NIO：Non IO 同步非阻塞 IO，是传统 IO 的升级，客户端和服务器端通过 Channel（通道）通讯，实现了多路复用。
- AIO：Asynchronous IO 是 NIO 的升级，也叫 NIO2，实现了异步非堵塞 IO ，异步 IO 的操作基于事件和回调机制。

- BIO (Blocking I/O): 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。
- NIO (New I/O): NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发
- AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。

#### 必读有关“为何选择Netty”的11个疑问及解答
- <http://www.52im.net/forum.php?mod=viewthread&tid=163&highlight=netty>

#### TCP网关
HAProxy nginx LVS
- 生产环境大部分还是采用通过rest方式获取IpList，然后有客户端直接发起长连接的方式
- 京东京麦的生产级TCP网关技术实践总结 <http://www.52im.net/forum.php?mod=viewthread&tid=1243&highlight=netty>
- 一套海量在线用户的移动端IM架构设计实践 <http://www.52im.net/thread-812-1-1.html>

#### Why Netty?JDK 原生 NIO 程序的问题
- JDK 原生也有一套网络应用程序 API，但是存在一系列问题，主要如下：
    1. NIO 的类库和 API 繁杂，使用麻烦：你需要熟练掌握 Selector、ServerSocketChannel、SocketChannel、ByteBuffer 等。
    1. 需要具备其他的额外技能做铺垫：例如熟悉 Java 多线程编程，因为 NIO 编程涉及到 Reactor 模式，你必须对多线程和网路编程非常熟悉，才能编写出高质量的 NIO 程序。
    1. 可靠性能力补齐，开发工作量和难度都非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常码流的处理等等。NIO 编程的特点是功能开发相对容易，但是可靠性能力补齐工作量和难度都非常大。
    1. JDK NIO 的 Bug：例如臭名昭著的 Epoll Bug，它会导致 Selector 空轮询，最终导致 CPU 100%。官方声称在 JDK 1.6 版本的 update 18 修复了该问题，但是直到 JDK 1.7 版本该问题仍旧存在，只不过该 Bug 发生概率降低了一些而已，它并没有被根本解决。
    
#### Java NIO epoll bug 以及 Netty 的解决之道
- epoll 空轮询导致 CPU 利用率 100% <http://songkun.me/2019/07/26/2019-07-26-java-nio-epoll-bug-and-netty-solution/>
 
#### netty中的epoll实现
- 在java中，IO多路复用的功能通过nio中的Selector提供，在不同的操作系统下jdk会通过spi的方式加载不同的实现，
比如在macos下是KQueueSelectorProvider，KQueueSelectorProvider底层使用了kqueue来进行IO多路复用；
在linux 2.6以后的版本则是EPollSelectorProvider，EPollSelectorProvider底层使用的是epoll。
虽然jdk自身提供了selector的epoll实现，netty仍实现了自己的epoll版本，根据netty开发者在StackOverflow的回答，主要原因有两个：
    - 支持更多socket option，比如TCP_CORK和SO_REUSEPORT
    - 使用了边缘触发（ET）模式
- <https://juejin.im/post/5d46ce64f265da03e05af722>
- ET和LT的区别在于触发事件的条件不同，LT比较符合编程思维（有满足条件的就触发），ET触发的条件更苛刻一些（仅在发生变化时才触发），对使用者的要求也更高，理论效率更高
- 边缘触发和水平触发<https://juejin.im/post/5cdaa67f518825691b4a5cc0>

#### Netty Coding

- EchoServer

```java
package com.siglea.bobo.imserver.service;

import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioServerSocketChannel;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.net.InetSocketAddress;

@Service("EchoServer")
public class EchoServer
{
    // 服务器端口
    @Value("${server.port}")
    private int port;
    // 通过nio方式来接收连接和处理连接
    private static EventLoopGroup boss = new NioEventLoopGroup();
    private static EventLoopGroup work = new NioEventLoopGroup();

    // 启动引导器
    private static ServerBootstrap b = new ServerBootstrap();
    @Autowired
    private EchoServerHandler echoServerHandler;

    public void run()
    {
        try
        {
            b.group(boss, work);
            // 设置nio类型的channel
            b.channel(NioServerSocketChannel.class);
            // 设置监听端口
            b.localAddress(new InetSocketAddress(port));
            // 设置通道初始化
            b.childHandler(new ChannelInitializer<SocketChannel>()
            {
                //有连接到达时会创建一个channel
                protected void initChannel(SocketChannel ch) throws Exception
                {
                    // pipeline管理channel中的Handler
                    // 在channel队列中添加一个handler来处理业务
                    ch.pipeline().addLast("echoServerHandler",echoServerHandler);
                }
            });
            // 配置完成，开始绑定server
            // 通过调用sync同步方法阻塞直到绑定成功

            ChannelFuture f = b.bind().sync();
            System.out.println(EchoServer.class.getName() +
                    " started and listen on " + f.channel().localAddress());

            // 监听服务器关闭事件
            // 应用程序会一直等待，直到channel关闭
            f.channel().closeFuture().sync();
        } catch (Exception e)
        {
            e.printStackTrace();
        } finally
        {
            // 关闭EventLoopGroup，释放掉所有资源包括创建的线程
            work.shutdownGracefully();
            boss.shutdownGracefully();
        }

    }
}
```

- EchoServerHandler

```java
package com.siglea.bobo.imserver.service;


import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;
import io.netty.util.CharsetUtil;
import io.netty.util.ReferenceCountUtil;
import org.springframework.stereotype.Service;

@Service("echoServerHandler")
public class EchoServerHandler extends ChannelInboundHandlerAdapter
{

    /**
     * 建立连接时，发送一条消息
     */
    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception
    {
        System.out.println("连接的客户端地址:" + ctx.channel().remoteAddress());
        super.channelActive(ctx);
    }

    public void channelRead(ChannelHandlerContext ctx, Object msg)
    {
        try
        {
            System.out.println("server received data :" + msg);
//            ctx.write(msg);//写回数据，
            ctx.write(Unpooled.copiedBuffer("I am Server", CharsetUtil.UTF_8));
        } catch (Exception e){
            e.printStackTrace();
        } finally {
            ReferenceCountUtil.release(msg);
        }
    }

    public void channelReadComplete(ChannelHandlerContext ctx)
    {
        //flush掉所有写回的数据
        ctx.writeAndFlush(Unpooled.EMPTY_BUFFER)
                .addListener(ChannelFutureListener.CLOSE); //当flush完成后关闭channel
    }

    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)
    {
        //捕捉异常信息
        cause.printStackTrace();
        //出现异常时关闭channel
        ctx.close();
    }
}
```

- EchoClient

```java
package com.siglea.bobo.imserver.service;


import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;
import io.netty.util.CharsetUtil;
import io.netty.util.ReferenceCountUtil;
import org.springframework.stereotype.Service;

@Service("echoServerHandler")
public class EchoServerHandler extends ChannelInboundHandlerAdapter
{

    /**
     * 建立连接时，发送一条消息
     */
    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception
    {
        System.out.println("连接的客户端地址:" + ctx.channel().remoteAddress());
        super.channelActive(ctx);
    }

    public void channelRead(ChannelHandlerContext ctx, Object msg)
    {
        try
        {
            System.out.println("server received data :" + msg);
//            ctx.write(msg);//写回数据，
            ctx.write(Unpooled.copiedBuffer("I am Server", CharsetUtil.UTF_8));
        } catch (Exception e){
            e.printStackTrace();
        } finally {
            ReferenceCountUtil.release(msg);
        }
    }

    public void channelReadComplete(ChannelHandlerContext ctx)
    {
        //flush掉所有写回的数据
        ctx.writeAndFlush(Unpooled.EMPTY_BUFFER)
                .addListener(ChannelFutureListener.CLOSE); //当flush完成后关闭channel
    }

    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)
    {
        //捕捉异常信息
        cause.printStackTrace();
        //出现异常时关闭channel
        ctx.close();
    }
}
```

- EchoClientHandler

```java
package com.siglea.bobo.imclient.service;


import io.netty.buffer.ByteBuf;
import io.netty.buffer.ByteBufUtil;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;
import io.netty.util.CharsetUtil;
import io.netty.util.ReferenceCountUtil;
import org.springframework.stereotype.Service;

@Service("echoClientHandler")
public class EchoClientHandler extends ChannelInboundHandlerAdapter
{
    /**
     * 此方法会在连接到服务器后被调用
     */
    public void channelActive(ChannelHandlerContext ctx)
    {

        System.out.println("channelActive");
        ctx.write(Unpooled.copiedBuffer("Netty rocks!", CharsetUtil.UTF_8));
        ctx.flush();
    }

    /**
     * 业务逻辑处理
     */
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception
    {
        System.out.println("channelRead");
        // 如果不是protobuf类型的数据
        if (!(msg instanceof ByteBuf))
        {
            System.out.println("未知数据!" + msg);
            return;
        }
        try
        {
            ByteBuf in = (ByteBuf) msg;
            System.out.println("Client received: " +
                    ByteBufUtil.hexDump(in.readBytes(in.readableBytes())));
        } catch (Exception e)
        {
            e.printStackTrace();
        } finally
        {
            ReferenceCountUtil.release(msg);
        }
    }
    /**
     * 捕捉到异常
     */
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)
    {
        System.out.println("exceptionCaught");
        cause.printStackTrace();
        ctx.close();
    }
}
```

#### Netty & Protobuf

```java
bootstrap.handler(
        new ChannelInitializer<SocketChannel>()
        {
            public void initChannel(SocketChannel ch) throws Exception
            {
                ch.pipeline().addLast(new ProtobufDecoder());
                ch.pipeline().addLast(new ProtobufEncoder());
                ch.pipeline().addLast(chatClientHandler);
​
            }
        }
);
```

#### 再说Netty 的零拷⻉
##### 传统意义的拷⻉
是在发送数据的时候，传统的实现方式是:
1. `File.read(bytes)`
2. `Socket.send(bytes)` 这种方式需要四次数据拷⻉和四次上下文切换: 1. 数据从磁盘读取到内核的read buffer
2. 数据从内核缓冲区拷⻉到用戶缓冲区
3. 数据从用戶缓冲区拷⻉到内核的socket buffer
4. 数据从内核的socket buffer拷⻉到网卡接口(硬件)的缓冲区
##### 零拷⻉的概念
明显上面的第二步和第三步是没有必要的，通过java的FileChannel.transferTo方法，可以避免上面两次多余的拷⻉(当然这需要底层操作 系统支持)
1. 调用transferTo,数据从文件由DMA引擎拷⻉到内核read buffer 2. 接着DMA从内核read buffer将数据拷⻉到网卡接口buffer 上面的两次操作都不需要CPU参与，所以就达到了零拷⻉。 3、Netty中的零拷⻉
主要体现在三个方面:
1、bytebuffer
Netty发送和接收消息主要使用bytebuffer，bytebuffer使用对外内存(DirectMemory)直接进行Socket读写。
原因:如果使用传统的堆内存进行Socket读写，JVM会将堆内存buffer拷⻉一份到直接内存中然后再写入socket，多了一次缓冲区的内存拷 ⻉。DirectMemory中可以直接通过DMA发送到网卡接口
##### Composite Buffers
传统的ByteBuffer，如果需要将两个ByteBuffer中的数据组合到一起，我们需要首先创建一个size=size1+size2大小的新的数组，然后将两 个数组中的数据拷⻉到新的数组中。但是使用Netty提供的组合ByteBuf，就可以避免这样的操作，因为CompositeByteBuf并没有真正将多 个Buffer组合起来，而是保存了它们的引用，从而避免了数据的拷⻉，实现了零拷⻉。
##### 对于FileChannel.transferTo的使用 
Netty中使用了FileChannel的transferTo方法，该方法依赖于操作系统实现零拷⻉。

#### 看一看
- <http://www.52im.net/forum.php?mod=viewthread&tid=2768&highlight=netty>
- <http://www.52im.net/thread-2775-1-1.html>
- Netty精粹之JAVA NIO开发需要知道的 <https://my.oschina.net/andylucc/blog/614295>
- 通俗易懂 <http://www.52im.net/forum.php?mod=viewthread&tid=2768&highlight=netty>
- 基于Netty实现海量接入的推送服务技术要点 <http://www.52im.net/forum.php?mod=viewthread&tid=166&highlight=netty>
- 游戏服务 <https://www.jianshu.com/p/82212eb7d76c>
- Netty整合SpringBoot并使用Protobuf进行数据传输 <https://juejin.im/post/5bb596196fb9a05d0f16f006>

